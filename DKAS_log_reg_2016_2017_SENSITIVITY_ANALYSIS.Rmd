---
title: "Sensitivity Analysis"
author: "Bindoff, A."
output:
  github_document: default
---

`r Sys.Date()`


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
options(width = 100)
```

```{r, warning = F, message = F}
# libraries can be installed by running the command install.packages(c("dplyr", "lme4", "lsmeans", "lmerTest", "foreign", "ggplot2", "reshape2"))

library(lme4)
library(lsmeans)
library(ggplot2)
library(foreign)
library(reshape2)
library(dplyr)
library(knitr)
library(xtable)
library(latticeExtra)
library(scico)
library(viridis)
```

## {.tabset .tabset-fade}

### Background and Summary of Results

An anonymous reviewer helpfully raised a question about the choice of cut-off score used in the logistic regression. Investigators chose the 90th percentile of pre-test scores *before* the analysis because this score was deemed to represent a very high level of dementia knowledge within the community. However, the same argument could be made for any percentile in the neighbourhood of the 90th percentile and this *'researcher degree of freedom'* could potentially have an effect on the findings. 

In order to make a quick assessment of the sensitivity of the results to different cut-offs, models were fitted using the 50th, 80th, 85th, 90th, and 95th percentiles of pre-test scores.  

```{r}
load("dkas_2016_2017")
```
```{r, echo = TRUE}
quantile(filter(df0, time == "pre")$score, c(0.50, 0.80, 0.85, 0.90, 0.95))

```

We found that conclusions were robust to choice of these cut-offs.  


### 50th percentile

```{r}
q <- 34.5
df0$score <- df0$raw_score >=q
df0$score <- as.integer(df0$score)
```

Fit the logistic regression model, obtain p-values using a likelihood-ratio test.  

```{r, eval = T, echo = T}
library(optimx)
m2 <- glmer(score ~ time + education + exposure + exposure:time +
              (1|subjectid) + (1|year), data = df0,
             family = binomial(link = logit), 
          control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

drop1(m2, test = "Chisq")
```

```{r}
plot(lsmeans(m2, ~ education + exposure))

```

### 80th percentile

```{r}
q <- 42
df0$score <- df0$raw_score >=q
df0$score <- as.integer(df0$score)
```


Fit the logistic regression model, obtain p-values using a likelihood-ratio test.  

```{r, eval = T, echo = T}
m2 <- glmer(score ~ time + education + exposure + exposure:time +
              (1|subjectid) + (1|year), data = df0,
             family = binomial(link = logit), 
          control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

drop1(m2, test = "Chisq")
```


```{r}
plot(lsmeans(m2, ~ education + exposure))

```

### 85th percentile

```{r}
q <- 44
df0$score <- df0$raw_score >=q
df0$score <- as.integer(df0$score)
```


Fit the logistic regression model, obtain p-values using a likelihood-ratio test.  

```{r, eval = T, echo = T}
m2 <- glmer(score ~ time + education + exposure + exposure:time +
              (1|subjectid) + (1|year), data = df0,
             family = binomial(link = logit), 
          control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

drop1(m2, test = "Chisq")
```

```{r}
plot(lsmeans(m2, ~ education + exposure))

```

### 90th percentile

```{r}
q <- 45
df0$score <- df0$raw_score >=q
df0$score <- as.integer(df0$score)
```


Fit the logistic regression model, obtain p-values using a likelihood-ratio test.  

```{r, eval = T, echo = T}
m2 <- glmer(score ~ time + education + exposure + exposure:time +
              (1|subjectid) + (1|year), data = df0,
             family = binomial(link = logit), 
          control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

drop1(m2, test = "Chisq")
```

```{r}
plot(lsmeans(m2, ~ education + exposure))

```

### 95th percentile

```{r}
q <- 47
df0$score <- df0$raw_score >=q
df0$score <- as.integer(df0$score)
```


Fit the logistic regression model, obtain p-values using a likelihood-ratio test.  

```{r, eval = T, echo = T}
m2 <- glmer(score ~ time + education + exposure + exposure:time +
              (1|subjectid) + (1|year), data = df0,
             family = binomial(link = logit), 
          control = glmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))

drop1(m2, test = "Chisq")
```

```{r}
plot(lsmeans(m2, ~ education + exposure))

```
